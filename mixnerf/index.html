<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>MixNeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://shawn615.github.io/mixnerf/img/mixnerf_titlecard.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1783">
    <meta property="og:image:height" content="1619">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://shawn615.github.io/mixnerf">
    <meta property="og:title" content="MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs">
    <meta property="og:description" content="Neural Radiance Field (NeRF) has broken new ground in the novel view synthesis due to its simple concept and state-of-the-art quality. However, it suffers from severe performance degradation unless trained with a dense set of images with different camera poses, which hinders its practical applications. Although previous methods addressing this problem achieved promising results, they relied heavily on the additional training resources, which goes against the philosophy of sparse-input novel-view synthesis pursuing the training efficiency. In this work, we propose MixNeRF, an effective training strategy for novel view synthesis from sparse inputs by modeling a ray with a mixture density model. Our MixNeRF estimates the joint distribution of RGB colors along the ray samples by modeling it with mixture of distributions. We also propose a new task of ray depth estimation as a useful training objective, which is highly correlated with 3D scene geometry. Moreover, we remodel the colors with regenerated blending weights based on the estimated ray depth and further improves the robustness for colors and viewpoints. Our MixNeRF outperforms other state-of-the-art methods in various standard benchmarks with superior efficiency of training and inference.">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="MixNeRF: Modeling a Ray with Mixture Density for Novel View Synthesis from Sparse Inputs">
    <meta name="twitter:description" content="Neural Radiance Field (NeRF) has broken new ground in the novel view synthesis due to its simple concept and state-of-the-art quality. However, it suffers from severe performance degradation unless trained with a dense set of images with different camera poses, which hinders its practical applications. Although previous methods addressing this problem achieved promising results, they relied heavily on the additional training resources, which goes against the philosophy of sparse-input novel-view synthesis pursuing the training efficiency. In this work, we propose MixNeRF, an effective training strategy for novel view synthesis from sparse inputs by modeling a ray with a mixture density model. Our MixNeRF estimates the joint distribution of RGB colors along the ray samples by modeling it with mixture of distributions. We also propose a new task of ray depth estimation as a useful training objective, which is highly correlated with 3D scene geometry. Moreover, we remodel the colors with regenerated blending weights based on the estimated ray depth and further improves the robustness for colors and viewpoints. Our MixNeRF outperforms other state-of-the-art methods in various standard benchmarks with superior efficiency of training and inference.">
    <meta name="twitter:image" content="https://shawn615.github.io/mixnerf/img/mixnerf_titlecard.png">


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;ðŸ“Š&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>MixNeRF</b>: Modeling a Ray with Mixture Density <br> for Novel View Synthesis from Sparse Inputs<br>
                <small>
                    CVPR 2023 <font color="red">(Qualcomm Innovation Fellowship Korea 2023 Winner)</font>
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:1 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://shawn615.github.io/">
                              Seunghyeon Seo
                            </a>
                            <br>Seoul National University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://scholar.google.com/citations?hl=en&user=BKlC7TQAAAAJ">
                              Donghoon Han*
                            </a>
                            <br>Seoul National University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://yeonjin-chang.github.io/">
                              Yeonjin Chang*
                            </a>
                            <br>Seoul National University
                        </td>
                        <td>
                            <a style="text-decoration:none" href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">
                              Nojun Kwak
                            </a>
                            <br>Seoul National University
                        </td>
                    </tr>
                </table>
                *denotes equal contribution
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2302.08788">
                            <img src="./img/paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                       <li>
                           <a href="https://youtu.be/PXljJordbFk">
                           <img src="./img/youtube_icon.png" height="60px">
                               <h4><strong>Video</strong></h4>
                           </a>
                       </li>
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Shiny Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">-->
<!--                            <image src="img/real_database_icon.png" height="60px">-->
<!--                                <h4><strong>Real Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>                            -->
                        <li>
                            <a href="https://github.com/shawn615/MixNeRF" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <div class="video-compare-container" id="materialsDiv">-->
<!--                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>-->
<!--                    -->
<!--                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>-->
<!--                </div>-->
<!--			</div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
Neural Radiance Field (NeRF) has broken new ground in the novel view synthesis due to its simple concept and state-of-the-art quality. However, it suffers from severe performance degradation unless trained with a dense set of images with different camera poses, which hinders its practical applications. Although previous methods addressing this problem achieved promising results, they relied heavily on the additional training resources, which goes against the philosophy of sparse-input novel-view synthesis pursuing the training efficiency. In this work, we propose MixNeRF, an effective training strategy for novel view synthesis from sparse inputs by modeling a ray with a mixture density model. Our MixNeRF estimates the joint distribution of RGB colors along the ray samples by modeling it with mixture of distributions. We also propose a new task of ray depth estimation as a useful training objective, which is highly correlated with 3D scene geometry. Moreover, we remodel the colors with regenerated blending weights based on the estimated ray depth and further improves the robustness for colors and viewpoints. Our MixNeRF outperforms other state-of-the-art methods in various standard benchmarks with superior efficiency of training and inference.                </p>
            </div>
        </div>

       <div class="row">
           <div class="col-md-8 col-md-offset-2">
               <h3>
                   Video
               </h3>
               <div class="text-center">
                   <div style="position:relative;padding-top:56.25%;">
                       <iframe src="https://youtube.com/embed/PXljJordbFk" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                   </div>
               </div>
           </div>
       </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Modeling a Ray with Mixture Density Model
                </h3>
                <div class="text-justify">
                    First, our MixNeRF estimates the distribution of the RGB color values along the samples of the ray on a pixel with a mixture model, which is derived from a weighted combination of component distributions.
                    The conventional outputs of NeRF for each sampled point are used as a location parameter and to compute a mixing coefficient Ï€, respectively. In addition to these, a scale parameter Î² is also estimated in our model:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/mixture_1.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    The pdf of our mixture model formed by the component distributions above is defined as:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/mixture_2.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    The mixture coefficient Ï€ij is derived from the density output Ïƒij as follows:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/mixture_3.png" width="50%">
                </div>
<!--                <div class="text-center">-->
<!--                    <video id="refdir" width="40%" playsinline autoplay loop muted>-->
<!--                        <source src="video/reflection_animation.mp4" type="video/mp4" />-->
<!--                    </video>-->
<!--                </div>-->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Depth Estimation by Mixture Density Model
                </h3>
                <div class="text-justify">
                    We propose a rayâ€™s depth estimation as an effective auxiliary task for training our MixNeRF with sparse inputs.
                    Our MixNeRF estimates <i>d</i>, the rayâ€™s depth, which is defined as the length of the unnormalized ray direction vector along the ray samples.
                    The pdf of our mixture model for the depth of the <i>i</i>-th ray is as follows:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/raydepth_1.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    Since the mixing coefficient Ï€ and parameter Î² are optimized through the supervision of the depth as well as the color values, it improves the robustness of our MixNeRF for slight changes of geometry.
                    In addition, we exploit the estimated depth to regenerate the blending weights along the samples and model the RGB color values by a mixture of distributions once again.
                    Since the estimated depth of each sample is trained to be nearly identical to the ground truth depth, but not exactly the same, it can play a role of pseudo geometry for adjacent points of the sample without any additional pre-generation process of extra training data.
                    The new blending weights along a ray based on the estimated ray depths are defined as follow:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/raydepth_2.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    Finally, we model the color values along a ray based on the new mixing coefficients and the corresponding pdf is as follows:
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/raydepth_3.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    Since the estimated ray depths are likely to be close enough to those of the ground truths, we use the same GT color values of input rays for modeling the mixture distribution based on the newly generated mixing coefficients.
                    It further improves the robustness for shift of colors and ray viewpoints by simply modeling a ray once again with regenerated blending weights, eliminating pre-generation and extra inference of unseen views without much computational overhead.
                </div>
<!--                <div class="text-center">-->
<!--                    <video id="refdir" width="40%" playsinline autoplay loop muted>-->
<!--                        <source src="video/reflection_animation.mp4" type="video/mp4" />-->
<!--                    </video>-->
<!--                </div>-->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Benefit of Mixture Density Model
                </h3>
                <div class="text-center">
                    <img src="./img/weight_dist.png" width="100%">
                </div>
                <div class="text-justify">
                    For the unimodal distribution in blue, mip-NeRF does not estimate the mode well and achieves degenerate geometry.
                    However, RegNeRF and our MixNeRF show the unimodal weight distributions leading to higher-quality novel views, and especially our MixNeRF achieves the distribution with sharper mode than RegNeRF, which is more similar to that of mip-NeRF (All-view).
                    In case of the bimodal-shaped distribution in red, our MixNeRF estimates the weight distribution successfully while both mip-NeRF and RegNeRF fail to estimate the accurate modes.
                    Since the predicted 3D geometry is directly correlated with how well the density is estimated, our MixNeRF is able to learn the geometry more efficiently with limited input views through mixture density modeling.
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Depth Map Estimation
                </h3>
                <div class="video-compare-container" id="materialsDiv">
                    <video class="video" id="depth" loop playsinline autoPlay muted src="video/depth_regnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>
                    <canvas height=0 class="videoMerge" id="depthMerge"></canvas>
                </div>
                <div class="text-justify">
                    We observe that RegNeRF fails to learn the geometry with its smoothing strategy and achieves degenerate results due to the overly strong prior of depth smoothness.
                    However, since our MixNeRF learns the depth of a ray by leveraging a mixture density model without smoothing from additional unseen rays, the depth maps are predicted much more efficiently and precisely.
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Efficiency in Training and Inference
                </h3>
                <div class="text-center">
                    <img src="./img/efficiency_1.png" width="75%">
                </div>
                <div class="text-justify">
                    Although it takes a similar amount of time to train DietNeRF as MixNeRF, its performance is inferior significantly to ours in 3 and 6-view scenario.
                    Compared to RegNeRF, ours outperforms it with about 42% shorter training time per scene under the same number of input view scenario, resulting from the elimination of extra inference for additional unseen rays.
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <h4>
                    DTU 3-view
                </h4>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="33%">
                            <video id="dtu1" width="100%" playsinline autoplay loop muted>
                                <source src="video/results_dtu_1.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="33%">
                            <video id="dtu2" width="100%" playsinline autoplay loop muted>
                                <source src="video/results_dtu_2.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="33%">
                            <video id="dtu3" width="100%" playsinline autoplay loop muted>
                                <source src="video/results_dtu_3.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
                <br>
                <h4>
                    LLFF 3-view
                </h4>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <video id="llff1" width="100%" playsinline autoplay loop muted>
                                <source src="video/results_llff_1.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="50%">
                            <video id="llff2" width="100%" playsinline autoplay loop muted>
                                <source src="video/results_llff_2.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
                <br>
                <h4>
                    Blender 4-view
                </h4>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="50%">
                            <video id="blender1" width="100%" playsinline autoplay loop muted>
                                <source src="video/results_blender_1.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="50%">
                            <video id="blender2" width="100%" playsinline autoplay loop muted>
                                <source src="video/results_blender_2.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@InProceedings{Seo_2023_CVPR,
    author    = {Seo, Seunghyeon and Han, Donghoon and Chang, Yeonjin and Kwak, Nojun},
    title     = {MixNeRF: Modeling a Ray With Mixture Density for Novel View Synthesis From Sparse Inputs},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {20659-20668}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                This work was supported by NRF grant (2021R1A2C3006659) and IITP grants (2021-0-01343, 2022-0-00953) funded by Korean Government.
                </p>
            </div>
        </div>
    </div>


</body></html>
