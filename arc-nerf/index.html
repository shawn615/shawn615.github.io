<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ARC-NeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://shawn615.github.io/arcnerf/img/arcnerf_titlecard.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1783">
    <meta property="og:image:height" content="1619">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://shawn615.github.io/arcnerf">
    <meta property="og:title" content="ARC-NeRF: Area Ray Casting for Broader Unseen View Coverage in Few-shot Object Rendering">
    <meta property="og:description" content="Recent advancements in the Neural Radiance Field (NeRF) have enhanced its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge, often leading to artifacts and a lack of fine object details. Addressing this, we propose ARC-NeRF, an effective regularization-based approach with a novel Area Ray Casting strategy. While the previous ray augmentation methods are limited to covering only a single unseen view per extra ray, our proposed Area Ray covers a broader range of unseen views with just a single ray and enables an adaptive high-frequency regularization based on target pixel photo-consistency. Moreover, we propose luminance consistency regularization, which enhances the consistency of relative luminance between the original and Area Ray, leading to more accurate object textures. The relative luminance, as a free lunch extra data easily derived from RGB images, can be effectively utilized in few-shot scenarios where available training data is limited. Our ARC-NeRF outperforms its baseline and achieves competitive results on multiple benchmarks with sharply rendered fine details.">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="ARC-NeRF: Area Ray Casting for Broader Unseen View Coverage in Few-shot Object Rendering">
    <meta name="twitter:description" content="Recent advancements in the Neural Radiance Field (NeRF) have enhanced its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge, often leading to artifacts and a lack of fine object details. Addressing this, we propose ARC-NeRF, an effective regularization-based approach with a novel Area Ray Casting strategy. While the previous ray augmentation methods are limited to covering only a single unseen view per extra ray, our proposed Area Ray covers a broader range of unseen views with just a single ray and enables an adaptive high-frequency regularization based on target pixel photo-consistency. Moreover, we propose luminance consistency regularization, which enhances the consistency of relative luminance between the original and Area Ray, leading to more accurate object textures. The relative luminance, as a free lunch extra data easily derived from RGB images, can be effectively utilized in few-shot scenarios where available training data is limited. Our ARC-NeRF outperforms its baseline and achieves competitive results on multiple benchmarks with sharply rendered fine details.">
    <meta name="twitter:image" content="https://shawn615.github.io/arcnerf/img/arcnerf_titlecard.png">


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;ðŸªž&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>ARC-NeRF</b>: Area Ray Casting for Broader Unseen View Coverage<br>in Few-shot Object Rendering<br>
                <small>
                    CVPRW 2025: 4th edition of Computer Vision for Metaverse Workshop <font color="red">(Oral)</font>
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://shawn615.github.io/">
                              Seunghyeon Seo<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://yeonjin-chang.github.io/">
                              Yeonjin Chang<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://natureyoo.github.io/">
                              Jayeon Yoo<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://lifrary.github.io/">
                              Seungwoo Lee<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://hojunlee.info/">
                              Hojun Lee<sup>1,2</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://mipal.snu.ac.kr/members#h.qz2cc7ep0m80">
                              Nojun Kwak<sup>1</sup>
                            </a>
                        </td>
                    </tr>
                </table>
                <div style="margin-top: 8px; font-size: 14px;">
                    <sup>1</sup>Seoul National University &nbsp;&nbsp;
                    <sup>2</sup>Xperty Corp.
                </div>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2403.10906">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="https://youtu.be/qrdRH9irAlk">-->
<!--                            <img src="./img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Shiny Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">-->
<!--                            <image src="img/real_database_icon.png" height="60px">-->
<!--                                <h4><strong>Real Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>                            -->
                        <!-- <li>
                            <a href="https://shawn615.github.io/flipnerf" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code [Coming soon]</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <div class="video-compare-container" id="materialsDiv">-->
<!--                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>-->
<!--                    -->
<!--                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>-->
<!--                </div>-->
<!--			</div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="img/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
Recent advancements in the Neural Radiance Field (NeRF) have enhanced its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge, often leading to artifacts and a lack of fine object details. Addressing this, we propose ARC-NeRF, an effective regularization-based approach with a novel Area Ray Casting strategy. While the previous ray augmentation methods are limited to covering only a single unseen view per extra ray, our proposed Area Ray covers a broader range of unseen views with just a single ray and enables an adaptive high-frequency regularization based on target pixel photo-consistency. Moreover, we propose luminance consistency regularization, which enhances the consistency of relative luminance between the original and Area Ray, leading to more accurate object textures. The relative luminance, as a free lunch extra data easily derived from RGB images, can be effectively utilized in few-shot scenarios where available training data is limited. Our ARC-NeRF outperforms its baseline and achieves competitive results on multiple benchmarks with sharply rendered fine details.            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Area Ray Casting
                </h3>
                <div class="text-justify">
                    Compared to the existing ray augmentation schemes where a resulting augmented ray corresponds to an unseen view, our proposed Area Ray covers the area of continuous unseen views by Integrated Positional Encoding (IPE), providng more efficient extra training resources.
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/arearay_generation.png" width="75%">
                </div>
                <br>
                <div class="text-justify">
                    First, we reparameterize the metric distance \( t \in [t_{near}, t_{far}] \) as \( \tilde{t} \) to derive the variance \( \tilde{\sigma}^{2}_{\rho} \), which is perpendicular to the Area Ray (a).
                    Next, as shown in (b), we derive a base radius of the Area Ray \( \tilde{\rho} \) from the angle \( \tilde{\theta} \) between \( r \) and \( \hat{n} \) using the trigonometric function as follows:
                    $$\tilde{\rho} = \tilde{\delta}\tan\tilde{\theta},$$
                    where \( \tilde{\delta} = 1 - \tilde{t}_{s} \) so that \( \tilde{\rho} \) is obtained from the sample located on \( \tilde{t} = 1 \).
                    However, directly employing the obtained \( \tilde{\rho} \) in IPE results in significantly large \( \tilde{\sigma}^2_\rho \), leading to over-regularization of high-frequency components for the samples along an Area Ray.
                    Thus, we adjust the scale of \( \tilde{\rho} \) to \( [0, 1] \) to contract \( \tilde{\rho} \) with the large \( \tilde{\theta} \) value into a proper range, while leaving the one with small \( \tilde{\theta} \) affected little as follows:
                    $$\tilde{\rho} = \exp{(-1 / (\tilde{\delta}\tan{\tilde{\theta}}))}.$$
                </div>
                <br>
                <div class="text-justify">
                    And then, \( \tilde{\sigma}^2_\rho \) is derived from \( \tilde{t} \) and \( \tilde{\rho} \) to featurize the conical frustums of Area Ray as multivariate Gaussian by simply replacing the original metric distance \( t \), which is used in mip-NeRF, with \( \tilde{t} \) as follows:
                    $$\tilde{\sigma}^2_\rho = \tilde{\rho}^2\left( \frac{\tilde{t}^2_\mu}{4} + \frac{5\tilde{t}^2_\delta}{12} - \frac{4\tilde{t}^4_\delta}{15(3\tilde{t}^2_\mu + \tilde{t}^2_\delta)} \right),$$
                    where \( \tilde{t}_\delta \) and \( \tilde{t}_\mu \) denote a half-width and mid-point of adjacent \( \tilde{t} \) values.
                    Note that we use the same \( \mu_t \) and \( \sigma^2_t \) for the mean and variance along the Area Ray as mip-NeRF.
                </div>
                <br>
                <div class="text-justify">
                    Finally, we generate an Area Ray \( \mathbf{\tilde{r}}(t) = \mathbf{\tilde{o}} + t\mathbf{\tilde{d}} \), where \( \mathbf{\tilde{d}} = -\mathbf{\hat{n}} \) and \( \mathbf{\tilde{o}} = \mathbf{p}_s - t_s\mathbf{\tilde{d}} \), so that the Area Ray is cast from the newly set camera origin \( \mathbf{\tilde{o}} \), which has the same distance from \( \mathbf{p}_s \) as the original ray, covering the unseen view area between the original ray and the corresponding reflection ray around the axis of \( \mathbf{\hat{n}} \).
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Luminance Consistency Regularization
                </h3>
                <div class="text-center">
                    <img src="./img/luminance.png" width="100%">
                </div>
                <div class="text-justify">
                    We propose the luminance map as an effective additional training resource for few-shot scenarios with limited data, providing 'free lunch' information easily derived from RGB images, and introduce luminance consistency regularization.
                    <br>
                    For simplicity, we use a relative luminance value, which is normalized as \( [0, 1] \), and derive the GT relative luminance \( y_\text{GT} \) of a target pixel as follows:
                    $$y_\text{GT} = \sum_{\bar{c}}^{\{\bar{r}, \bar{g}, \bar{b}\}} \lambda_{\bar{c}} \bar{c},$$
                    where \( \bar{c} = c_\text{GT}^{2.2} \) indicates a linear rgb component converted from the gamma-compressed one by applying a simple power curve.
                </div>
                <br>
                <div class="text-justify">
                    In addition to the existing outputs, our ARC-NeRF estimates the luminance \( y \) as additional outputs per sample along a ray and renders the final luminance \( \hat{y} \) by volume rendering as follows:
                    $$\hat{y}(\mathbf{r}) = \sum_{i=1}^{N}w_i y_i,$$
                    where \( y_i \in [0, 1] \) is the estimated relative luminance of the \( i \)-th sample along a ray \( \mathbf{r} \).
                </div>
                <br>
                <h2>
                    MixNeRF
                </h2>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_mix1" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_scan34_3view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_mix2" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_scan34_3view_rgbstd.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_mix3" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_scan34_3view_depth.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_mix4" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_scan34_3view_depthstd.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
                <h2>
                    FlipNeRF
                </h2>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_flip1" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_scan34_3view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_flip2" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_scan34_3view_rgbstd.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_flip3" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_scan34_3view_depth.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="ue_flip4" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_scan34_3view_depthstd.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
                <div class="text-justify">
                    With our proposed UE Loss, ours improves both of the rendering quality and the reliability of the model outputs by a large margin compared to MixNeRF.
                    (From the left to the right: RGB, RGB_Std, Depth, Depth_Std)
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Bottleneck Feature Consistency
                </h3>
                <div class="text-justify">
                    We encourage the consistency of bottleneck feature distributions between r and râ€², which are intermediate feature vectors, i.e., outputs of the spatial MLP of NeRF, by Jensen-Shannon Divergence (JSD):
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/bfcloss_1.png" width="50%">
                </div>
                <br>
                <div class="text-justify">
                    where Ïˆ(Â·), b and bâ€² denote the softmax function, the bottleneck features of r and râ€², respectively.
                    We regulate the pair of features effectively by enhancing consistency between bottleneck features without depending on additional feature extractors.
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Comparison with Baselines
                </h3>
                <div class="text-justify">
                    Our FlipNeRF estimates more accurate surface normals than other baselines, leading to the performance gain with better reconstructed fine details from limited input views.
                    (From the left to the right: mip-NeRF, Ref-NeRF, MixNeRF, FlipNeRF (Ours))
                </div>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="25%">
                            <video id="rgb1" width="100%" playsinline autoplay loop muted>
                                <source src="video/mipnerf_hotdog_4view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="rgb2" width="100%" playsinline autoplay loop muted>
                                <source src="video/refnerf_hotdog_4view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="rgb3" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_hotdog_4view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="rgb4" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_hotdog_4view_rgb.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
                <br>
                <table width="100%">
                    <tr>
                        <td align="left" valign="top" width="25%">
                            <video id="normals1" width="100%" playsinline autoplay loop muted>
                                <source src="video/mipnerf_hotdog_4view_normals.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="normals2" width="100%" playsinline autoplay loop muted>
                                <source src="video/refnerf_hotdog_4view_normals.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="normals3" width="100%" playsinline autoplay loop muted>
                                <source src="video/mixnerf_hotdog_4view_normals.mp4" type="video/mp4" />
                            </video>
                        </td>
                        <td align="left" valign="top" width="25%">
                            <video id="normals4" width="100%" playsinline autoplay loop muted>
                                <source src="video/flipnerf_hotdog_4view_normals.mp4" type="video/mp4" />
                            </video>
                        </td>
                    </tr>
                </table>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                        @article{seo2023flipnerf,
                        title={FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis},
                        author={Seo, Seunghyeon and Chang, Yeonjin and Kwak, Nojun},
                        journal={arXiv preprint arXiv:2306.17723},
                        year={2023}}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                This work was supported by NRF grant (2021R1A2C3006659) and IITP grant (RS-2021-II211343), both funded by MSIT of the Korean Government. The work was also supported by Samsung Electronics (IO201223-08260-01).
                </p>
            </div>
        </div>
    </div>


</body></html>
