<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ARC-NeRF</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://shawn615.github.io/arcnerf/img/arcnerf_titlecard.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1783">
    <meta property="og:image:height" content="1619">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://shawn615.github.io/arcnerf">
    <meta property="og:title" content="ARC-NeRF: Area Ray Casting for Broader Unseen View Coverage in Few-shot Object Rendering">
    <meta property="og:description" content="Recent advancements in the Neural Radiance Field (NeRF) have enhanced its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge, often leading to artifacts and a lack of fine object details. Addressing this, we propose ARC-NeRF, an effective regularization-based approach with a novel Area Ray Casting strategy. While the previous ray augmentation methods are limited to covering only a single unseen view per extra ray, our proposed Area Ray covers a broader range of unseen views with just a single ray and enables an adaptive high-frequency regularization based on target pixel photo-consistency. Moreover, we propose luminance consistency regularization, which enhances the consistency of relative luminance between the original and Area Ray, leading to more accurate object textures. The relative luminance, as a free lunch extra data easily derived from RGB images, can be effectively utilized in few-shot scenarios where available training data is limited. Our ARC-NeRF outperforms its baseline and achieves competitive results on multiple benchmarks with sharply rendered fine details.">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="ARC-NeRF: Area Ray Casting for Broader Unseen View Coverage in Few-shot Object Rendering">
    <meta name="twitter:description" content="Recent advancements in the Neural Radiance Field (NeRF) have enhanced its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge, often leading to artifacts and a lack of fine object details. Addressing this, we propose ARC-NeRF, an effective regularization-based approach with a novel Area Ray Casting strategy. While the previous ray augmentation methods are limited to covering only a single unseen view per extra ray, our proposed Area Ray covers a broader range of unseen views with just a single ray and enables an adaptive high-frequency regularization based on target pixel photo-consistency. Moreover, we propose luminance consistency regularization, which enhances the consistency of relative luminance between the original and Area Ray, leading to more accurate object textures. The relative luminance, as a free lunch extra data easily derived from RGB images, can be effectively utilized in few-shot scenarios where available training data is limited. Our ARC-NeRF outperforms its baseline and achieves competitive results on multiple benchmarks with sharply rendered fine details.">
    <meta name="twitter:image" content="https://shawn615.github.io/arcnerf/img/arcnerf_titlecard.png">


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;⌛️&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>ARC-NeRF</b>: Area Ray Casting for Broader Unseen View Coverage<br>in Few-shot Object Rendering<br>
                <small>
                    CVPRW 2025: 4th edition of Computer Vision for Metaverse Workshop <font color="red">(Oral)</font>
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://shawn615.github.io/">
                              Seunghyeon Seo<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://yeonjin-chang.github.io/">
                              Yeonjin Chang<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://natureyoo.github.io/">
                              Jayeon Yoo<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://lifrary.github.io/">
                              Seungwoo Lee<sup>1</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://hojunlee.info/">
                              Hojun Lee<sup>1,2</sup>
                            </a>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://mipal.snu.ac.kr/members#h.qz2cc7ep0m80">
                              Nojun Kwak<sup>1</sup>
                            </a>
                        </td>
                    </tr>
                </table>
                <div style="margin-top: 8px; font-size: 14px;">
                    <sup>1</sup>Seoul National University &nbsp;&nbsp;
                    <sup>2</sup>Xperty Corp.
                </div>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2403.10906">
                            <img src="./img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="https://youtu.be/qrdRH9irAlk">-->
<!--                            <img src="./img/youtube_icon.png" height="60px">-->
<!--                                <h4><strong>Video</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref.zip" target="_blank">-->
<!--                            <image src="img/database_icon.png" height="60px">-->
<!--                                <h4><strong>Shiny Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">-->
<!--                            <image src="img/real_database_icon.png" height="60px">-->
<!--                                <h4><strong>Real Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>                            -->
                        <!-- <li>
                            <a href="https://shawn615.github.io/flipnerf" target="_blank">
                            <image src="img/github.png" height="60px">
                                <h4><strong>Code [Coming soon]</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <div class="video-compare-container" id="materialsDiv">-->
<!--                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>-->
<!--                    -->
<!--                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>-->
<!--                </div>-->
<!--			</div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h2>
                    Abstract
                </h2>
                <image src="img/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
Recent advancements in the Neural Radiance Field (NeRF) have enhanced its capabilities for novel view synthesis, yet its reliance on dense multi-view training images poses a practical challenge, often leading to artifacts and a lack of fine object details. Addressing this, we propose ARC-NeRF, an effective regularization-based approach with a novel Area Ray Casting strategy. While the previous ray augmentation methods are limited to covering only a single unseen view per extra ray, our proposed Area Ray covers a broader range of unseen views with just a single ray and enables an adaptive high-frequency regularization based on target pixel photo-consistency. Moreover, we propose luminance consistency regularization, which enhances the consistency of relative luminance between the original and Area Ray, leading to more accurate object textures. The relative luminance, as a free lunch extra data easily derived from RGB images, can be effectively utilized in few-shot scenarios where available training data is limited. Our ARC-NeRF outperforms its baseline and achieves competitive results on multiple benchmarks with sharply rendered fine details.            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h2>
                    Area Ray Casting
                </h2>
                <div class="text-justify">
                    Compared to the existing ray augmentation schemes where a resulting augmented ray corresponds to an unseen view, our proposed Area Ray covers the area of continuous unseen views by Integrated Positional Encoding (IPE), providng more efficient extra training resources.
                </div>
                <br>
                <div class="text-center">
                    <img src="./img/arearay_generation.png" width="75%">
                </div>
                <br>
                <div class="text-justify">
                    First, we reparameterize the metric distance \( t \in [t_{near}, t_{far}] \) as \( \tilde{t} \) to derive the variance \( \tilde{\sigma}^{2}_{\rho} \), which is perpendicular to the Area Ray (a).
                    Next, as shown in (b), we derive a base radius of the Area Ray \( \tilde{\rho} \) from the angle \( \tilde{\theta} \) between \( r \) and \( \hat{n} \) using the trigonometric function as follows:
                    $$\tilde{\rho} = \tilde{\delta}\tan\tilde{\theta},$$
                    where \( \tilde{\delta} = 1 - \tilde{t}_{s} \) so that \( \tilde{\rho} \) is obtained from the sample located on \( \tilde{t} = 1 \).
                    However, directly employing the obtained \( \tilde{\rho} \) in IPE results in significantly large \( \tilde{\sigma}^2_\rho \), leading to over-regularization of high-frequency components for the samples along an Area Ray.
                    Thus, we adjust the scale of \( \tilde{\rho} \) to \( [0, 1] \) to contract \( \tilde{\rho} \) with the large \( \tilde{\theta} \) value into a proper range, while leaving the one with small \( \tilde{\theta} \) affected little as follows:
                    $$\tilde{\rho} = \exp{(-1 / (\tilde{\delta}\tan{\tilde{\theta}}))}.$$
                </div>
                <br>
                <div class="text-justify">
                    And then, \( \tilde{\sigma}^2_\rho \) is derived from \( \tilde{t} \) and \( \tilde{\rho} \) to featurize the conical frustums of Area Ray as multivariate Gaussian by simply replacing the original metric distance \( t \), which is used in mip-NeRF, with \( \tilde{t} \) as follows:
                    $$\tilde{\sigma}^2_\rho = \tilde{\rho}^2\left( \frac{\tilde{t}^2_\mu}{4} + \frac{5\tilde{t}^2_\delta}{12} - \frac{4\tilde{t}^4_\delta}{15(3\tilde{t}^2_\mu + \tilde{t}^2_\delta)} \right),$$
                    where \( \tilde{t}_\delta \) and \( \tilde{t}_\mu \) denote a half-width and mid-point of adjacent \( \tilde{t} \) values.
                    Note that we use the same \( \mu_t \) and \( \sigma^2_t \) for the mean and variance along the Area Ray as mip-NeRF.
                </div>
                <br>
                <div class="text-justify">
                    Finally, we generate an Area Ray \( \mathbf{\tilde{r}}(t) = \mathbf{\tilde{o}} + t\mathbf{\tilde{d}} \), where \( \mathbf{\tilde{d}} = -\mathbf{\hat{n}} \) and \( \mathbf{\tilde{o}} = \mathbf{p}_s - t_s\mathbf{\tilde{d}} \), so that the Area Ray is cast from the newly set camera origin \( \mathbf{\tilde{o}} \), which has the same distance from \( \mathbf{p}_s \) as the original ray, covering the unseen view area between the original ray and the corresponding reflection ray around the axis of \( \mathbf{\hat{n}} \).
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h2>
                    Luminance Consistency Regularization
                </h2>
                <div class="text-center">
                    <img src="./img/luminance.png" width="100%">
                </div>
                <br>
                <div class="text-justify">
                    We propose the luminance map as an effective additional training resource for few-shot scenarios with limited data, providing 'free lunch' information easily derived from RGB images, and introduce luminance consistency regularization.
                    <br>
                    For simplicity, we use a relative luminance value, which is normalized as \( [0, 1] \), and derive the GT relative luminance \( y_\text{GT} \) of a target pixel as follows:
                    $$y_\text{GT} = \sum_{\bar{c}}^{\{\bar{r}, \bar{g}, \bar{b}\}} \lambda_{\bar{c}} \bar{c},$$
                    where \( \bar{c} = c_\text{GT}^{2.2} \) indicates a linear rgb component converted from the gamma-compressed one by applying a simple power curve.
                </div>
                <br>
                <div class="text-justify">
                    In addition to the existing outputs, our ARC-NeRF estimates the luminance \( y \) as additional outputs per sample along a ray and renders the final luminance \( \hat{y} \) by volume rendering as follows:
                    $$\hat{y}(\mathbf{r}) = \sum_{i=1}^{N}w_i y_i,$$
                    where \( y_i \in [0, 1] \) is the estimated relative luminance of the \( i \)-th sample along a ray \( \mathbf{r} \).
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h2>
                    Experiments
                </h2>
                <h3>
                    Frequency regularization effect of Area Ray
                </h3>
                <div class="text-center">
                    <img src="./img/freq_reg.png" width="75%">
                </div>
                <br>
                <div class="text-justify">
                    Compared to FreeNeRF which forcibly masks the high-frequency spectrum in the early training phase, ours adaptively regularizes the high-frequency components of additional ray samples based on the target pixel photo-consistency (i.e., the angle between the original ray and Area Ray) during the whole training process.
                    As a result, our ARC-NeRF already achieves sharper fine details at 25K iteration than the fully trained FreeNeRF.
                </div>
                <br>
                <h3>
                    Effectiveness of Area Ray as a bundle of rays
                </h3>
                <div class="text-center">
                    <img src="./img/multicasting.png" width="75%">
                </div>
                <br>
                <div class="text-justify">
                    Our ARC-NeRF outperforms FlipNeRF in all scenarios by a large margin.
                    The training time per scene is measured using the same GPU, iterations, and batch size.
                    The size of circles is proportional to \( \kappa \), i.e., the number of augmented rays per original ray.
                </div>
                <br>
                <h3>
                    Comparison with Other Baselines
                </h3>
                <div class="text-justify">
                    Our ARC-NeRF achieves competitive rendering quality with better capturing fine details.
                </div>
                <br>
                <div class="text-center">
                    <video id="blender1" width="100%" playsinline autoplay loop muted>
                        <source src="video/realistic-synthetic_4view_comparison_video.mp4" type="video/mp4" />
                    </video>
                    4-view
                </div>
                <br>
                <div class="text-center">
                    <video id="blender2" width="100%" playsinline autoplay loop muted>
                        <source src="video/realistic-synthetic_8view_comparison_video.mp4" type="video/mp4" />
                    </video>
                    8-view
                </div>
                <br>
                <div class="text-justify">
                    FreeNeRF is trained with the black and white prior assuming the estimated black and white color as the background and table, respectively, which is a highly strong assumption specific to the dataset, and achieves degenerate results without the prior.
                    However, our ARC-NeRF achieves competitive performance without any heuristic prior by using Area Ray, which enables adaptive regularization of high-frequency.
                </div>
                <br>
                <div class="text-center">
                    <video id="dtu1" width="75%" playsinline autoplay loop muted>
                        <source src="video/dtu_3view_freenerf-ours_video.mp4" type="video/mp4" />
                    </video>
                    <br>
                    3-view; Notable improvement in the detail of the tail.
                </div>
                <br>
                <div class="text-center">
                    <video id="dtu2" width="75%" playsinline autoplay loop muted>
                        <source src="video/dtu_6view_freenerf-ours_video.mp4" type="video/mp4" />
                    </video>
                    <br>
                    6-view; Apple surface textures are more stably reconstructed across changing views.
                </div>
                <br>
                <div class="text-center">
                    <video id="dtu3" width="75%" playsinline autoplay loop muted>
                        <source src="video/dtu_9view_freenerf-ours_video.mp4" type="video/mp4" />
                    </video>
                    <br>
                    9-view; Brick textures are also more consistently reproduced.
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{seo2024hourglassnerf,
  title={HourglassNeRF: Casting an Hourglass as a Bundle of Rays for Few-shot Neural Rendering},
  author={Seo, Seunghyeon and Chang, Yeonjin and Yoo, Jayeon and Lee, Seungwoo and Lee, Hojun and Kwak, Nojun},
  journal={arXiv preprint arXiv:2403.10906},
  year={2024}
}
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                This work was supported by NRF grant (2021R1A2C3006659) and IITP grant (RS-2021-II211343), both funded by MSIT of the Korean Government. The work was also supported by Samsung Electronics (IO201223-08260-01).
                </p>
            </div>
        </div>
    </div>


</body></html>
